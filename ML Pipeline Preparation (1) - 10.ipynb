{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy in /opt/conda/lib/python3.6/site-packages (1.19.5)\r\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "!pip install numpy --upgrade\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('message_and_categories_ds10',)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 \n",
    "con = sqlite3.connect(\"DisasterResponse10.db\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related :  ['1' '0']\n",
      "request :  ['0' '1']\n",
      "offer :  ['0' '1']\n",
      "aid_related :  ['0' '1']\n",
      "medical_help :  ['0' '1']\n",
      "medical_products :  ['0' '1']\n",
      "search_and_rescue :  ['0' '1']\n",
      "security :  ['0' '1']\n",
      "military :  ['0' '1']\n",
      "child_alone :  ['0']\n",
      "water :  ['0' '1']\n",
      "food :  ['0' '1']\n",
      "shelter :  ['0' '1']\n",
      "clothing :  ['0' '1']\n",
      "money :  ['0' '1']\n",
      "missing_people :  ['0' '1']\n",
      "refugees :  ['0' '1']\n",
      "death :  ['0' '1']\n",
      "other_aid :  ['0' '1']\n",
      "infrastructure_related :  ['0' '1']\n",
      "transport :  ['0' '1']\n",
      "buildings :  ['0' '1']\n",
      "electricity :  ['0' '1']\n",
      "tools :  ['0' '1']\n",
      "hospitals :  ['0' '1']\n",
      "shops :  ['0' '1']\n",
      "aid_centers :  ['0' '1']\n",
      "other_infrastructure :  ['0' '1']\n",
      "weather_related :  ['0' '1']\n",
      "floods :  ['0' '1']\n",
      "storm :  ['0' '1']\n",
      "fire :  ['0' '1']\n",
      "earthquake :  ['0' '1']\n",
      "cold :  ['0' '1']\n",
      "other_weather :  ['0' '1']\n",
      "direct_report :  ['0' '1']\n",
      "0        Weather update - a cold front from Cuba that c...\n",
      "1                  Is the Hurricane over or is it not over\n",
      "2                          Looking for someone but no name\n",
      "3        UN reports Leogane 80-90 destroyed. Only Hospi...\n",
      "4        says: west side of Haiti, rest of the country ...\n",
      "5                   Information about the National Palace-\n",
      "6                           Storm at sacred heart of jesus\n",
      "7        Please, we need tents and water. We are in Sil...\n",
      "8          I would like to receive the messages, thank you\n",
      "9        I am in Croix-des-Bouquets. We have health iss...\n",
      "10       There's nothing to eat and water, we starving ...\n",
      "11       I am in Petionville. I need more information r...\n",
      "12       I am in Thomassin number 32, in the area named...\n",
      "13       Let's do it together, need food in Delma 75, i...\n",
      "14       More information on the 4636 number in order f...\n",
      "15       A Comitee in Delmas 19, Rue ( street ) Janvier...\n",
      "16       We need food and water in Klecin 12. We are dy...\n",
      "17       are you going to call me or do you want me to ...\n",
      "18          I don't understand how to use this thing 4636.\n",
      "19       I would like to know if the earthquake is over...\n",
      "20       I would like to know if one of the radio ginen...\n",
      "21                          I'm in Laplaine, I am a victim\n",
      "22       There's a lack of water in Moleya, please info...\n",
      "23       Those people who live at Sibert need food they...\n",
      "24       I want to say hello, my message is to let you ...\n",
      "25                      Can you tell me about this service\n",
      "26       People I'm at Delma 2, we don't anything what ...\n",
      "27       We are at Gressier we needs assistance right a...\n",
      "28       How can we get water and food in Fontamara 43 ...\n",
      "29       We need help. Carrefour has been forgotten com...\n",
      "                               ...                        \n",
      "25998    The ability to pick dengue from influenza is c...\n",
      "25999    A Federation chartered ship arrived from Lae w...\n",
      "26000    The result is that in Aceh province many prefa...\n",
      "26001    Otherwise, the risk is families fleeing again ...\n",
      "26002    A United Nations team from the Electoral Assis...\n",
      "26003    Senegal and Guinea-Bissau have agreed to condu...\n",
      "26004    The President said that her Government always ...\n",
      "26005    It was decided that all vehicle movement from ...\n",
      "26006    The tendency to link deforestation with large ...\n",
      "26007    Polio is a viral disease that attacks the nerv...\n",
      "26008    The new constitution declares that 'Somalia is...\n",
      "26009    We're providing clean water to people who woul...\n",
      "26010    Relief items include towels, sanitary napkins,...\n",
      "26011    In Aceh's Meulaboh town the UN refugee agency ...\n",
      "26012    WHO is recruiting a sanitary engineer / consul...\n",
      "26013    Following the severe floods which occurred ove...\n",
      "26014    The closure has stopped 169 inbound flights an...\n",
      "26015    BANGKOK, 24 January 2012 (NNT) - Prime Ministe...\n",
      "26016    Cadmium, a metallic element widely used in bat...\n",
      "26017    Epidemic surveillance: National Institute of C...\n",
      "26018    2.1 Due to sporadic skirmishes in eastern D.R....\n",
      "26019    No other army had gone to greater lengths to a...\n",
      "26020    The delivery was made in conjunction with the ...\n",
      "26021    However while ECOWAS wanted him to lead a 12-m...\n",
      "26022    Hpakant, an area rich with coveted jade stones...\n",
      "26023    The training demonstrated how to enhance micro...\n",
      "26024    A suitable candidate has been selected and OCH...\n",
      "26025    Proshika, operating in Cox's Bazar municipalit...\n",
      "26026    Some 2,000 women protesting against the conduc...\n",
      "26027    A radical shift in thinking came about as a re...\n",
      "Name: message, Length: 26028, dtype: object\n",
      "      related request offer aid_related medical_help medical_products  \\\n",
      "0           1       0     0           0            0                0   \n",
      "1           1       0     0           1            0                0   \n",
      "2           1       0     0           0            0                0   \n",
      "3           1       1     0           1            0                1   \n",
      "4           1       0     0           0            0                0   \n",
      "5           0       0     0           0            0                0   \n",
      "6           1       0     0           0            0                0   \n",
      "7           1       1     0           1            0                0   \n",
      "8           0       0     0           0            0                0   \n",
      "9           1       1     0           1            1                1   \n",
      "10          1       1     0           1            1                1   \n",
      "11          0       0     0           0            0                0   \n",
      "12          1       1     0           1            0                0   \n",
      "13          1       1     0           1            0                0   \n",
      "14          1       0     0           0            0                0   \n",
      "15          1       1     0           1            0                1   \n",
      "16          1       1     0           1            1                0   \n",
      "17          0       0     0           0            0                0   \n",
      "18          0       0     0           0            0                0   \n",
      "19          1       0     0           0            0                0   \n",
      "20          1       0     0           0            0                0   \n",
      "21          1       1     0           1            0                0   \n",
      "22          1       1     0           1            0                0   \n",
      "23          1       1     0           1            0                0   \n",
      "24          1       0     0           0            0                0   \n",
      "25          0       0     0           0            0                0   \n",
      "26          1       1     0           1            1                1   \n",
      "27          1       1     0           1            1                0   \n",
      "28          1       1     0           1            0                0   \n",
      "29          1       1     0           1            0                0   \n",
      "...       ...     ...   ...         ...          ...              ...   \n",
      "25998       1       0     0           1            1                0   \n",
      "25999       1       0     0           1            0                1   \n",
      "26000       1       0     0           1            0                0   \n",
      "26001       0       0     0           0            0                0   \n",
      "26002       0       0     0           0            0                0   \n",
      "26003       1       0     0           1            0                0   \n",
      "26004       1       0     0           0            0                0   \n",
      "26005       1       0     0           1            0                0   \n",
      "26006       1       0     0           0            0                0   \n",
      "26007       0       0     0           0            0                0   \n",
      "26008       1       0     0           0            0                0   \n",
      "26009       1       0     0           1            0                0   \n",
      "26010       1       0     0           1            0                1   \n",
      "26011       1       0     0           1            0                0   \n",
      "26012       0       0     0           0            0                0   \n",
      "26013       1       0     0           0            0                0   \n",
      "26014       1       0     0           0            0                0   \n",
      "26015       1       0     0           1            0                0   \n",
      "26016       0       0     0           0            0                0   \n",
      "26017       1       0     0           1            1                1   \n",
      "26018       1       0     0           1            0                0   \n",
      "26019       1       1     0           0            0                0   \n",
      "26020       1       0     0           0            0                0   \n",
      "26021       0       0     0           0            0                0   \n",
      "26022       1       0     0           0            0                0   \n",
      "26023       0       0     0           0            0                0   \n",
      "26024       0       0     0           0            0                0   \n",
      "26025       1       0     0           0            0                0   \n",
      "26026       1       0     0           1            0                0   \n",
      "26027       1       0     0           0            0                0   \n",
      "\n",
      "      search_and_rescue security military child_alone      ...       \\\n",
      "0                     0        0        0           0      ...        \n",
      "1                     0        0        0           0      ...        \n",
      "2                     0        0        0           0      ...        \n",
      "3                     0        0        0           0      ...        \n",
      "4                     0        0        0           0      ...        \n",
      "5                     0        0        0           0      ...        \n",
      "6                     0        0        0           0      ...        \n",
      "7                     0        0        0           0      ...        \n",
      "8                     0        0        0           0      ...        \n",
      "9                     0        0        0           0      ...        \n",
      "10                    0        0        0           0      ...        \n",
      "11                    0        0        0           0      ...        \n",
      "12                    0        0        0           0      ...        \n",
      "13                    0        0        0           0      ...        \n",
      "14                    0        0        0           0      ...        \n",
      "15                    0        0        0           0      ...        \n",
      "16                    0        0        0           0      ...        \n",
      "17                    0        0        0           0      ...        \n",
      "18                    0        0        0           0      ...        \n",
      "19                    0        0        0           0      ...        \n",
      "20                    0        0        0           0      ...        \n",
      "21                    0        0        0           0      ...        \n",
      "22                    0        0        0           0      ...        \n",
      "23                    0        0        0           0      ...        \n",
      "24                    0        0        0           0      ...        \n",
      "25                    0        0        0           0      ...        \n",
      "26                    0        0        0           0      ...        \n",
      "27                    0        0        0           0      ...        \n",
      "28                    0        0        0           0      ...        \n",
      "29                    0        0        0           0      ...        \n",
      "...                 ...      ...      ...         ...      ...        \n",
      "25998                 0        0        0           0      ...        \n",
      "25999                 0        0        0           0      ...        \n",
      "26000                 0        0        0           0      ...        \n",
      "26001                 0        0        0           0      ...        \n",
      "26002                 0        0        0           0      ...        \n",
      "26003                 0        0        1           0      ...        \n",
      "26004                 0        0        0           0      ...        \n",
      "26005                 0        0        1           0      ...        \n",
      "26006                 0        0        0           0      ...        \n",
      "26007                 0        0        0           0      ...        \n",
      "26008                 0        0        0           0      ...        \n",
      "26009                 1        0        0           0      ...        \n",
      "26010                 0        0        0           0      ...        \n",
      "26011                 0        0        0           0      ...        \n",
      "26012                 0        0        0           0      ...        \n",
      "26013                 0        0        0           0      ...        \n",
      "26014                 0        0        0           0      ...        \n",
      "26015                 0        0        0           0      ...        \n",
      "26016                 0        0        0           0      ...        \n",
      "26017                 0        0        0           0      ...        \n",
      "26018                 0        0        0           0      ...        \n",
      "26019                 0        0        0           0      ...        \n",
      "26020                 0        0        0           0      ...        \n",
      "26021                 0        0        0           0      ...        \n",
      "26022                 0        0        0           0      ...        \n",
      "26023                 0        0        0           0      ...        \n",
      "26024                 0        0        0           0      ...        \n",
      "26025                 0        0        0           0      ...        \n",
      "26026                 0        0        1           0      ...        \n",
      "26027                 0        0        0           0      ...        \n",
      "\n",
      "      aid_centers other_infrastructure weather_related floods storm fire  \\\n",
      "0               0                    0               0      0     0    0   \n",
      "1               0                    0               1      0     1    0   \n",
      "2               0                    0               0      0     0    0   \n",
      "3               0                    0               0      0     0    0   \n",
      "4               0                    0               0      0     0    0   \n",
      "5               0                    0               0      0     0    0   \n",
      "6               0                    0               1      0     1    0   \n",
      "7               0                    0               0      0     0    0   \n",
      "8               0                    0               0      0     0    0   \n",
      "9               0                    0               0      0     0    0   \n",
      "10              0                    1               1      1     0    0   \n",
      "11              0                    0               0      0     0    0   \n",
      "12              0                    0               0      0     0    0   \n",
      "13              0                    0               0      0     0    0   \n",
      "14              0                    0               0      0     0    0   \n",
      "15              0                    0               0      0     0    0   \n",
      "16              0                    0               0      0     0    0   \n",
      "17              0                    0               0      0     0    0   \n",
      "18              0                    0               0      0     0    0   \n",
      "19              0                    0               1      0     0    0   \n",
      "20              0                    0               0      0     0    0   \n",
      "21              0                    0               0      0     0    0   \n",
      "22              0                    0               0      0     0    0   \n",
      "23              0                    0               0      0     0    0   \n",
      "24              0                    0               0      0     0    0   \n",
      "25              0                    0               0      0     0    0   \n",
      "26              0                    0               0      0     0    0   \n",
      "27              0                    0               1      1     0    0   \n",
      "28              0                    0               0      0     0    0   \n",
      "29              0                    0               0      0     0    0   \n",
      "...           ...                  ...             ...    ...   ...  ...   \n",
      "25998           0                    0               0      0     0    0   \n",
      "25999           0                    0               0      0     0    0   \n",
      "26000           0                    0               0      0     0    0   \n",
      "26001           0                    0               0      0     0    0   \n",
      "26002           0                    0               0      0     0    0   \n",
      "26003           0                    0               0      0     0    0   \n",
      "26004           0                    0               0      0     0    0   \n",
      "26005           0                    0               0      0     0    0   \n",
      "26006           0                    0               1      1     0    1   \n",
      "26007           0                    0               0      0     0    0   \n",
      "26008           0                    0               0      0     0    0   \n",
      "26009           0                    0               0      0     0    0   \n",
      "26010           0                    0               0      0     0    0   \n",
      "26011           0                    0               0      0     0    0   \n",
      "26012           0                    0               0      0     0    0   \n",
      "26013           0                    0               1      1     0    0   \n",
      "26014           0                    0               0      0     0    0   \n",
      "26015           0                    0               1      1     0    0   \n",
      "26016           0                    0               0      0     0    0   \n",
      "26017           0                    0               0      0     0    0   \n",
      "26018           0                    0               0      0     0    0   \n",
      "26019           0                    0               0      0     0    0   \n",
      "26020           0                    0               0      0     0    0   \n",
      "26021           0                    0               0      0     0    0   \n",
      "26022           0                    0               0      0     0    0   \n",
      "26023           0                    0               0      0     0    0   \n",
      "26024           0                    0               0      0     0    0   \n",
      "26025           0                    0               0      0     0    0   \n",
      "26026           0                    0               0      0     0    0   \n",
      "26027           0                    0               0      0     0    0   \n",
      "\n",
      "      earthquake cold other_weather direct_report  \n",
      "0              0    0             0             0  \n",
      "1              0    0             0             0  \n",
      "2              0    0             0             0  \n",
      "3              0    0             0             0  \n",
      "4              0    0             0             0  \n",
      "5              0    0             0             0  \n",
      "6              0    0             0             0  \n",
      "7              0    0             0             1  \n",
      "8              0    0             0             0  \n",
      "9              0    0             0             1  \n",
      "10             0    0             0             1  \n",
      "11             0    0             0             0  \n",
      "12             0    0             0             1  \n",
      "13             0    0             0             1  \n",
      "14             0    0             0             0  \n",
      "15             0    0             0             1  \n",
      "16             0    0             0             1  \n",
      "17             0    0             0             0  \n",
      "18             0    0             0             0  \n",
      "19             1    0             0             0  \n",
      "20             0    0             0             0  \n",
      "21             0    0             0             1  \n",
      "22             0    0             0             1  \n",
      "23             0    0             0             0  \n",
      "24             0    0             0             0  \n",
      "25             0    0             0             0  \n",
      "26             0    0             0             1  \n",
      "27             0    0             0             1  \n",
      "28             0    0             0             1  \n",
      "29             0    0             0             1  \n",
      "...          ...  ...           ...           ...  \n",
      "25998          0    0             0             0  \n",
      "25999          0    0             0             0  \n",
      "26000          0    0             0             0  \n",
      "26001          0    0             0             0  \n",
      "26002          0    0             0             0  \n",
      "26003          0    0             0             0  \n",
      "26004          0    0             0             0  \n",
      "26005          0    0             0             0  \n",
      "26006          0    0             1             0  \n",
      "26007          0    0             0             0  \n",
      "26008          0    0             0             0  \n",
      "26009          0    0             0             1  \n",
      "26010          0    0             0             1  \n",
      "26011          0    0             0             0  \n",
      "26012          0    0             0             0  \n",
      "26013          0    0             0             0  \n",
      "26014          0    0             0             0  \n",
      "26015          0    0             0             0  \n",
      "26016          0    0             0             0  \n",
      "26017          0    0             0             0  \n",
      "26018          0    0             0             0  \n",
      "26019          0    0             0             1  \n",
      "26020          0    0             0             0  \n",
      "26021          0    0             0             0  \n",
      "26022          0    0             0             0  \n",
      "26023          0    0             0             0  \n",
      "26024          0    0             0             0  \n",
      "26025          0    0             0             0  \n",
      "26026          0    0             0             0  \n",
      "26027          0    0             0             0  \n",
      "\n",
      "[26028 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# load data from database\n",
    "#df = cursor.execute(\"SELECT * FROM message_and_categories_ds4 \")\n",
    "#engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('message_and_categories_ds10', con = 'sqlite:///DisasterResponse10.db')\n",
    "df.to_csv('ML_Data.csv')\n",
    "df.head()\n",
    "X = df.loc[:,[\"message\"]]\n",
    "X = X.squeeze()\n",
    "Y = df.iloc[:,list(range(36))]\n",
    "for column in Y.columns:\n",
    "    print(column, ': ',  Y[column].unique())\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tokenize(text):\n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    " # Stem word tokens and remove stop words\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    stemmed = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return stemmed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" Normalize text string, tokenize text string and remove stop words from text string\n",
    "    Args: \n",
    "        Text string with message\n",
    "    Returns \n",
    "        Normalized text string with word tokens \n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_tokens = []\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stemmed = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function: tokenize the text\n",
    "    Args:  source string\n",
    "    Return:\n",
    "    clean_tokens(str list): clean string list\n",
    "    \n",
    "    \"\"\"\n",
    "    #normalize text\n",
    "    text = re.sub(r'[^a-zA-Z0-9]',' ',text.lower())\n",
    "    \n",
    "    #token messages\n",
    "    words = word_tokenize(text)\n",
    "    tokens = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    #sterm and lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    \n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('Tfidf', TfidfTransformer()), \n",
    "    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23330    This was not only in preparation for expected ...\n",
      "13120    Saudi Public Assistance for Pakisatan Earthqua...\n",
      "11890    blackout in chile. nearly the whole country. s...\n",
      "12713    Baldwin Center in Stratford. Is without power ...\n",
      "11660    The ISS will pass over Santiago at mag 0.9 at ...\n",
      "8729     Each one hour ask favour, pardon, mercy,for ha...\n",
      "6587     I don't find nothing: covers and so one.Can yo...\n",
      "13973    The immigrant populations are likely to be hig...\n",
      "5647                   I want to know the info about 4636 \n",
      "20940    Parents hesitated to send children to school o...\n",
      "25720    As of this year, radical groups now controlled...\n",
      "10731    Wand to donate jackets , hot meals , non-peris...\n",
      "13689    The floods, which began in late July after hea...\n",
      "16422    With flood-hit families whose houses were tota...\n",
      "3406     Hospital Medecin Sans Frontiers (Doctors witho...\n",
      "17348    His club and others in the Eastern High Sierra...\n",
      "7930        what problem who contribute at a earth quake. \n",
      "18334    We are trying to preseve catchment areas but a...\n",
      "23523    The most common disease triggering an alert wa...\n",
      "10200    O catholic relief services Haiti hit by larges...\n",
      "19350    Logistics companies Agility, TNT, UPS and DHL ...\n",
      "17371         A cool spring also reduced glacier melt off.\n",
      "14875    The most critical gap in the whole assistance ...\n",
      "1868     Are the earthquakes over? and if they happen a...\n",
      "25683    It proposes assistance for veterinary services...\n",
      "23065    Associated activities regarding well construct...\n",
      "24140    Tuesday's 7.6-Richter-scale shockwave sheared ...\n",
      "6639     we wait for your answer we're hungry in Jacmel...\n",
      "1691             i need more information on the earthquake\n",
      "3563     Each hour or two ask for the grace of God, for...\n",
      "                               ...                        \n",
      "11742      Visit to Dichato Succesful! http://post.ly/RiEc\n",
      "17137    In a dilapidated community hall not far from t...\n",
      "19433    Funds may be applied toward monetary grants to...\n",
      "24535    World Vision will finish the Thailand Tsunami ...\n",
      "16946    Another 10,000 people were displaced in Tripur...\n",
      "22926    The International Committee of the Red Cross i...\n",
      "24477    So, as we weigh the dangers of this dreaded di...\n",
      "4764     i am in the mountains in lavout there is no wa...\n",
      "19946    But at around 3am, a Good Samaritan rescued th...\n",
      "8444     There are people who say that in the same way ...\n",
      "18900    Violence is especially on the increase in Andh...\n",
      "2962     Food is needed in Tabarre 43, Tapage Street, P...\n",
      "12645    The stock exchange was closed today, those guy...\n",
      "21758    We're providing clean water to people who woul...\n",
      "21780    * 386,093 people (85,975 families) affected * ...\n",
      "25778    The Red Cross has provided more than 125,000 f...\n",
      "3462     We are in Muller, near Carrefour-Feuilles. Cou...\n",
      "10989    @AP we are flooded in the other ZoneA that eve...\n",
      "7751     WHERE I FIND THAT FOOD? WE TELL ME TO WASH MY ...\n",
      "16332    The main foods observed in preparation were a ...\n",
      "20609    Excreta have found their way into drinking wat...\n",
      "144      continues previous truncated message : regardi...\n",
      "21440    The organisation operates with a not-for-profi...\n",
      "19279    Embankments, bridges, roads, railways, ferry t...\n",
      "7813     We need the psychologist concerning the progra...\n",
      "10955    I am a cook . I can prepare large amounts of f...\n",
      "17289    According to cited scientists, 20 percent of t...\n",
      "5192     I'll call you this week to give a response to ...\n",
      "12172    We have peaches garden which consist of 500 tr...\n",
      "235      We need food and water at Croix des bouquets a...\n",
      "Name: message, Length: 18219, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1 )\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics(actual, predicted, col_names):\n",
    "    \"\"\"Calculate evaluation metrics for ML model\n",
    "    \n",
    "    Args:\n",
    "    actual: array. Array containing actual labels.\n",
    "    predicted: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the predicted fields.\n",
    "       \n",
    "    Returns:\n",
    "    metrics_df: dataframe. Dataframe containing the accuracy, precision, recall \n",
    "    and f1 score for a given set of actual and predicted labels.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(actual[:, i], predicted[:, i])\n",
    "        precision = precision_score(actual[:, i], predicted[:, i], average = 'weighted')\n",
    "        recall = recall_score(actual[:, i], predicted[:, i], average ='weighted')\n",
    "        f1 = f1_score(actual[:, i], predicted[:, i], average = 'weighted')\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Create dataframe containing metrics\n",
    "    metrics = np.array(metrics)\n",
    "    metrics_df = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return metrics_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for training set\n",
    "y_train_pred = pipeline.predict(x_train)\n",
    "col_names = list(Y.columns.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_eval_metrics(np.array(y_train), y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "y_test_pred = pipeline.predict(x_test)\n",
    "\n",
    "eval_metrics0 = get_eval_metrics(np.array(y_test), y_test_pred, col_names)\n",
    "print(eval_metrics0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation the proportion of each column that have label == 1\n",
    "#Y.sum()/len(Y)\n",
    "#print(Y.sum())\n",
    "#print(len(Y))\n",
    "#type(Y)\n",
    "#(Y>2).count()\n",
    "#Y[Y >2].count()\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate median F1 score for all of the output classifiers\n",
    "    \n",
    "    Args:\n",
    "    y_true: array. Array containing actual labels.\n",
    "    y_pred: array. Array containing predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    score: float. Median F1 score for all of the output classifiers\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(y_pred)[1]):\n",
    "        f1 = f1_score(np.array(y_true)[:, i], y_pred[:, i])\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search object\n",
    "## Parameters for random forest\n",
    "'''\n",
    "\n",
    "parameters = {'vect__min_df': [1, 5],\n",
    "              'tfidf__use_idf':[True, False],\n",
    "              'clf__estimator__n_estimators':[10, 25], \n",
    "              'clf__estimator__min_samples_split':[2, 5, 10]}\n",
    "\n",
    "scorer = make_scorer(performance_metric)\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search object\n",
    "# Parameters for Decision Tree\n",
    "parameters = {'vect__min_df': [1, 5],\n",
    "              'Tfidf__use_idf':[True, False],\n",
    "              'clf__estimator__criterion':[2, 5]}\n",
    "\n",
    "scorer = make_scorer(performance_metric)\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 10, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best parameters\n",
    "np.random.seed(81)\n",
    "tuned_model = cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results of grid search\n",
    "tuned_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best mean test score\n",
    "np.max(tuned_model.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for best mean test score\n",
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test = tuned_model.predict(X_test)\n",
    "\n",
    "eval_metrics1 = get_eval_metrics(np.array(Y_test), tuned_pred_test, col_names)\n",
    "\n",
    "print(eval_metrics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary stats for first model\n",
    "eval_metrics0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary stats for tuned model\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using SVM instead of Random Forest Classifier\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(SVC()))\n",
    "])\n",
    "\n",
    "parameters2 = {'vect__min_df': [5],\n",
    "              'tfidf__use_idf':[True],\n",
    "              'clf__estimator__kernel': ['poly'], \n",
    "              'clf__estimator__degree': [1, 2, 3],\n",
    "              'clf__estimator__C':[1, 10, 100]}\n",
    "\n",
    "cv2 = GridSearchCV(pipeline2, param_grid = parameters2, scoring = scorer, verbose = 10)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(81)\n",
    "tuned_model2 = cv2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results of grid search\n",
    "tuned_model2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test2 = tuned_model2.predict(X_test)\n",
    "\n",
    "eval_metrics2 = get_eval_metrics(np.array(Y_test), tuned_pred_test2, col_names)\n",
    "\n",
    "print(eval_metrics2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle best model\n",
    "pickle.dump(tuned_model, open('disaster_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
